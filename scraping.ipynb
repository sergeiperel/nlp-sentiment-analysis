{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e271ba98-2115-485d-8055-dcad4fbc853c",
   "metadata": {},
   "source": [
    "### Data scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2d162e-2f6e-4f42-9b7b-4ae9bc7565c2",
   "metadata": {},
   "source": [
    "**Описание**\n",
    "\n",
    "Архитектура -- 3 слоя, если бы это была система баз данных. На уровне ERD. Точную дату все равно вытащить можно только по ссылке."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9547d7a-afdd-4e8a-8132-f6cb3d258bd7",
   "metadata": {},
   "source": [
    "**Импорт зависимостей**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb239e64-4d21-4a65-b2df-1afe140b1b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/safaridriver --enable\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from getpass import getpass\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad612b1-c65c-4603-bfd5-f6644b1ce07b",
   "metadata": {},
   "source": [
    "**Сбор ссылок**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9900bdb3-f0c8-4fa6-9032-1abe5104f056",
   "metadata": {},
   "source": [
    "Открываем список твитов Илона Маска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5887ca2a-66c9-4ee6-9d58-dd86324066fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input login:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input password:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Safari()  # Запускаем драйвер\n",
    "\n",
    "# Открываем нашу страницу:\n",
    "driver.get('https://x.com/search?q=from%3Aelonmusk&src=typed_query')  # TODO: можно добавить и date range\n",
    "time.sleep(7)  # let it load\n",
    "\n",
    "# Для начала вводим логин:\n",
    "print('input login:')\n",
    "driver.find_elements(by=By.XPATH, value='.//input[@autocomplete=\"username\"]')[0].click()\n",
    "driver.find_elements(by=By.XPATH, value='.//input[@autocomplete=\"username\"]')[0].send_keys(getpass())\n",
    "\n",
    "# Нажимаем на третью кнопку для перехода к вводу пароля:\n",
    "driver.find_elements(by=By.XPATH, value='.//button')[2].click()\n",
    "time.sleep(3)\n",
    "\n",
    "# Вводим пароль в появившемся втором поле для этого:\n",
    "print('input password:')\n",
    "driver.find_elements(by=By.XPATH, value='.//input')[1].click()\n",
    "driver.find_elements(by=By.XPATH, value='.//input')[1].send_keys(getpass())\n",
    "\n",
    "# Нажимаем кнопку залогиниться:\n",
    "driver.find_elements(by=By.XPATH, value='.//button')[3].click()\n",
    "time.sleep(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5759b323-2417-48ec-8993-b16f712d8d78",
   "metadata": {},
   "source": [
    "Получили список твитов Илона Маска. Теперь сохраним ссылку на каждый твит. Для этого ищем ссылку + скроллим дальше. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f7541b-198f-4171-858e-956223597065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 good links found on 1th step\n",
      "16 good links found on 2th step\n",
      "26 good links found on 3th step\n",
      "31 good links found on 4th step\n",
      "41 good links found on 5th step\n",
      "52 good links found on 6th step\n",
      "61 good links found on 7th step\n",
      "69 good links found on 8th step\n",
      "74 good links found on 9th step\n",
      "78 good links found on 10th step\n",
      "81 good links found on 11th step\n",
      "84 good links found on 12th step\n",
      "86 good links found on 13th step\n",
      "90 good links found on 14th step\n",
      "94 good links found on 15th step\n",
      "96 good links found on 16th step\n",
      "102 good links found on 17th step\n",
      "105 good links found on 18th step\n",
      "109 good links found on 19th step\n",
      "112 good links found on 20th step\n",
      "113 good links found on 21th step\n",
      "113 good links found on 22th step\n",
      "CPU times: user 1.72 s, sys: 292 ms, total: 2.01 s\n",
      "Wall time: 2min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tweets_links = []\n",
    "prev_len = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    driver.execute_script('window.scrollBy(0, 10000)')  # скроллим вниз\n",
    "    time.sleep(7)\n",
    "\n",
    "    cur_tweets_links_elements = driver.find_elements(by=By.XPATH, value='.//div/a')\n",
    "\n",
    "    # TODO. Почему tweets_links.extend(list(map(lambda x : x.get_attribute('href'), tweets_links))) отказался работать?\n",
    "    for cur_el in cur_tweets_links_elements:\n",
    "        tweets_links.append(cur_el.get_attribute('href'))\n",
    "\n",
    "    # how many good links we found:\n",
    "    good_links = [l for l in tweets_links if 'status' in l and 'elonmusk' in l and 'photo' not in l and 'analytics' not in l]\n",
    "    cur_len = len(list(set(good_links)))\n",
    "    print(f'{cur_len} good links found on {i + 1}th step')\n",
    "\n",
    "    # do we need a new scroll?\n",
    "    if cur_len == prev_len:\n",
    "        break\n",
    "    prev_len = len(list(set(good_links)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe37741-c318-42e9-852d-33a5cb886c64",
   "metadata": {},
   "source": [
    "Сохраняем результат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6607857d-afe3-49a7-b3ad-64cea42a948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.DataFrame(list(set(good_links)), columns=['tweet_link'])\n",
    "tweets_df['time'] = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89f029f7-0f27-4bbc-84bf-d1e06751f593",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tweets_df = pd.read_csv('tweet_links.csv').iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32fe09f3-7842-49db-9a5b-dc7d3734cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.concat([old_tweets_df, tweets_df]).reset_index(drop=True).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c2c8385-e805-44fd-986f-4828372914eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.reset_index(drop=True).drop_duplicates().to_csv('tweet_links.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e41e885-2ef2-4792-81b2-1a4782545682",
   "metadata": {},
   "source": [
    "**Сбор информации по ссылкам**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8adf781-2a60-4c0f-bc30-8eac78955f80",
   "metadata": {},
   "source": [
    "Теперь отдельно обрабатываем каждую ссылку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "79ce4e6e-1c9b-4a82-b0d4-2956edc9da24",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = pd.read_csv('tweet_links.csv').iloc[:,1].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4bba05-344d-4562-aa99-569370ee5876",
   "metadata": {},
   "source": [
    "Логинимся:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "af0d5359-a0ba-4453-beb9-7c1f80cf8e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input login:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input password:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Safari()  # Запускаем драйвер\n",
    "\n",
    "# Открываем нашу страницу:\n",
    "driver.get('https://x.com/search?q=from%3Aelonmusk&src=typed_query')  # TODO: можно добавить и date range\n",
    "time.sleep(7)  # let it load\n",
    "\n",
    "# Для начала вводим логин:\n",
    "print('input login:')\n",
    "driver.find_elements(by=By.XPATH, value='.//input[@autocomplete=\"username\"]')[0].click()\n",
    "driver.find_elements(by=By.XPATH, value='.//input[@autocomplete=\"username\"]')[0].send_keys(getpass())  # are\n",
    "\n",
    "# Нажимаем на третью кнопку для перехода к вводу пароля:\n",
    "driver.find_elements(by=By.XPATH, value='.//button')[2].click()\n",
    "time.sleep(3)\n",
    "\n",
    "# Вводим пароль в появившемся втором поле для этого:\n",
    "print('input password:')\n",
    "driver.find_elements(by=By.XPATH, value='.//input')[1].click()\n",
    "driver.find_elements(by=By.XPATH, value='.//input')[1].send_keys(getpass())\n",
    "\n",
    "# Нажимаем кнопку залогиниться:\n",
    "driver.find_elements(by=By.XPATH, value='.//button')[3].click()\n",
    "time.sleep(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0cf38c-445e-408e-bbcf-e4a09f4271c8",
   "metadata": {},
   "source": [
    "Собираем инфу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "d4580e3e-e5fb-4710-943c-50b117c9dbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 out of 128 done\n",
      "2 out of 128 done\n",
      "3 out of 128 done\n",
      "4 out of 128 done\n",
      "5 out of 128 done\n",
      "6 out of 128 done\n",
      "7 out of 128 done\n",
      "8 out of 128 done\n",
      "9 out of 128 done\n",
      "10 out of 128 done\n",
      "11 out of 128 done\n",
      "12 out of 128 done\n",
      "13 out of 128 done\n",
      "14 out of 128 done\n",
      "15 out of 128 done\n",
      "16 out of 128 done\n",
      "17 out of 128 done\n",
      "18 out of 128 done\n",
      "19 out of 128 done\n",
      "20 out of 128 done\n",
      "21 out of 128 done\n",
      "22 out of 128 done\n",
      "23 out of 128 done\n",
      "24 out of 128 done\n",
      "25 out of 128 done\n",
      "26 out of 128 done\n",
      "27 out of 128 done\n",
      "28 out of 128 done\n",
      "29 out of 128 done\n",
      "30 out of 128 done\n",
      "31 out of 128 done\n",
      "32 out of 128 done\n",
      "33 out of 128 done\n",
      "34 out of 128 done\n",
      "35 out of 128 done\n",
      "36 out of 128 done\n",
      "37 out of 128 done\n",
      "38 out of 128 done\n",
      "39 out of 128 done\n",
      "40 out of 128 done\n",
      "41 out of 128 done\n",
      "42 out of 128 done\n",
      "43 out of 128 done\n",
      "44 out of 128 done\n",
      "45 out of 128 done\n",
      "46 out of 128 done\n",
      "47 out of 128 done\n",
      "48 out of 128 done\n",
      "49 out of 128 done\n",
      "50 out of 128 done\n",
      "51 out of 128 done\n",
      "52 out of 128 done\n",
      "53 out of 128 done\n",
      "54 out of 128 done\n",
      "55 out of 128 done\n",
      "56 out of 128 done\n",
      "57 out of 128 done\n",
      "58 out of 128 done\n",
      "59 out of 128 done\n",
      "60 out of 128 done\n",
      "61 out of 128 done\n",
      "62 out of 128 done\n",
      "63 out of 128 done\n",
      "64 out of 128 done\n",
      "65 out of 128 done\n",
      "66 out of 128 done\n",
      "67 out of 128 done\n",
      "68 out of 128 done\n",
      "69 out of 128 done\n",
      "70 out of 128 done\n",
      "71 out of 128 done\n",
      "72 out of 128 done\n",
      "73 out of 128 done\n",
      "74 out of 128 done\n",
      "75 out of 128 done\n",
      "76 out of 128 done\n",
      "77 out of 128 done\n",
      "78 out of 128 done\n",
      "79 out of 128 done\n",
      "80 out of 128 done\n",
      "81 out of 128 done\n",
      "82 out of 128 done\n",
      "83 out of 128 done\n",
      "84 out of 128 done\n",
      "85 out of 128 done\n",
      "86 out of 128 done\n",
      "87 out of 128 done\n",
      "88 out of 128 done\n",
      "89 out of 128 done\n",
      "90 out of 128 done\n",
      "91 out of 128 done\n",
      "92 out of 128 done\n",
      "93 out of 128 done\n",
      "94 out of 128 done\n",
      "95 out of 128 done\n",
      "96 out of 128 done\n",
      "97 out of 128 done\n",
      "98 out of 128 done\n",
      "99 out of 128 done\n",
      "100 out of 128 done\n",
      "101 out of 128 done\n",
      "102 out of 128 done\n",
      "103 out of 128 done\n",
      "104 out of 128 done\n",
      "105 out of 128 done\n",
      "106 out of 128 done\n",
      "107 out of 128 done\n",
      "108 out of 128 done\n",
      "109 out of 128 done\n",
      "110 out of 128 done\n",
      "111 out of 128 done\n",
      "112 out of 128 done\n",
      "113 out of 128 done\n",
      "114 out of 128 done\n",
      "115 out of 128 done\n",
      "116 out of 128 done\n",
      "117 out of 128 done\n",
      "118 out of 128 done\n",
      "119 out of 128 done\n",
      "120 out of 128 done\n",
      "121 out of 128 done\n",
      "122 out of 128 done\n",
      "123 out of 128 done\n",
      "124 out of 128 done\n",
      "125 out of 128 done\n",
      "126 out of 128 done\n",
      "127 out of 128 done\n",
      "128 out of 128 done\n",
      "CPU times: user 2.62 s, sys: 1.05 s, total: 3.67 s\n",
      "Wall time: 16min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "scraping_res = dict()\n",
    "\n",
    "for link_num, link in enumerate(links):\n",
    "    driver.get(link)\n",
    "    time.sleep(7)\n",
    "\n",
    "    # Код ломается на твитах Маска внутри ветки, поэтому ищем индекс его твита:\n",
    "    branch_users = driver.find_elements(by=By.XPATH, value='.//div[@data-testid=\"User-Name\"]')\n",
    "    for branch_user_ind in range(len(driver.find_elements(by=By.XPATH, value='.//div[@data-testid=\"User-Name\"]'))):\n",
    "        _, user_login = driver.find_elements(\n",
    "            by=By.XPATH, value='.//div[@data-testid=\"User-Name\"]'\n",
    "        )[branch_user_ind].text.split('@')\n",
    "        if user_login == 'elonmusk':\n",
    "            break\n",
    "\n",
    "    # Все твиты в ветке имеют одну структуру, зная индекс, достаточно легко вытаскивает содержимое:\n",
    "    user_name, user_login = driver.find_elements(by=By.XPATH, value='.//div[@data-testid=\"User-Name\"]')[branch_user_ind].text.split('@')\n",
    "    tweet_text = driver.find_elements(by=By.XPATH, value='.//div[@data-testid=\"tweetText\"]')[branch_user_ind].text\n",
    "    tweet_datetime = driver.find_elements(by=By.XPATH, value='.//time')[branch_user_ind].text\n",
    "    statistics = driver.find_elements(by=By.XPATH, value='.//div[@role=\"group\"]')[branch_user_ind].get_attribute('aria-label')\n",
    "    \n",
    "    scraping_res[link] = (\n",
    "        user_name, \n",
    "        user_login, \n",
    "        tweet_text,\n",
    "        tweet_datetime,\n",
    "        statistics\n",
    "    )\n",
    "\n",
    "    print(f'{link_num + 1} out of {len(links)} done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31215d2d-6c5a-4fdb-88d1-af7dd262efe3",
   "metadata": {},
   "source": [
    "Сохраняем в формате нашего датасета. Сначала в нашем формате."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "4ed42c3c-8390-4c56-a581-0b53819f2d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    data=scraping_res.values(), \n",
    "    index=scraping_res.keys(),\n",
    "    columns=[\n",
    "        'user_name', \n",
    "        'user_login',\n",
    "        'tweet_text',\n",
    "        'tweet_datetime',\n",
    "        'statistics'\n",
    "    ]\n",
    ").to_csv('tweet_contents.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31cbd42-c9ef-40a9-8afe-9facc627e390",
   "metadata": {},
   "source": [
    "**Переводим в формат датасета**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987fb9cc-c20c-40aa-abb3-9fa3fcb3c0c3",
   "metadata": {},
   "source": [
    "Наконец, заключительный этап. Приводим данные к нужной нам структуре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d9c63e-b939-4acd-8b75-1352324fa510",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_df = pd.read_csv('tweet_contents.csv').rename(columns={'Unnamed: 0': 'link'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b070e10-b493-483a-8e83-874cac070238",
   "metadata": {},
   "source": [
    "Инициализируем колонки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "400be686-dd7b-4925-9871-8bc9850de622",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_df['user_name'] = scraped_df.user_name\n",
    "\n",
    "# Возьмем с сайта на текущий момент и введем вручную эти значения:\n",
    "scraped_df['user_location'] = np.nan\n",
    "scraped_df['user_description'] = np.nan\n",
    "scraped_df['user_created'] = '2009-06-02 20:12:29'\n",
    "scraped_df['user_followers'] = 210.1 * 1e6\n",
    "scraped_df['user_friends'] = 115\n",
    "scraped_df['user_favourites'] = 13503\n",
    "scraped_df['user_verified'] = True\n",
    "\n",
    "scraped_df['date'] = scraped_df['tweet_datetime']\n",
    "scraped_df['text'] = scraped_df.tweet_text\n",
    "\n",
    "# Дополнительная информация\n",
    "scraped_df['source'] = np.nan  # Не можем скачать source с помощью scraping\n",
    "\n",
    "# Preprocessing later\n",
    "scraped_df['hashtags'] = np.nan\n",
    "scraped_df['retweets'] = scraped_df.statistics\n",
    "scraped_df['favorites'] = scraped_df.statistics\n",
    "\n",
    "# No retweets\n",
    "scraped_df['is_retweet'] = False\n",
    "\n",
    "needed_cols = [\n",
    "    'user_name',\n",
    "    'user_location',\n",
    "    'user_description',\n",
    "    'user_created',\n",
    "    'user_followers',\n",
    "    'user_friends',\n",
    "    'user_favourites',\n",
    "    'user_verified',\n",
    "    'date',\n",
    "    'text',\n",
    "    'hashtags',\n",
    "    'source',\n",
    "    'retweets',\n",
    "    'favorites',\n",
    "    'is_retweet'\n",
    "]\n",
    "\n",
    "res_df = scraped_df.loc[:, needed_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022066d0-0ade-4950-aa85-6d6b7f8aaf9d",
   "metadata": {},
   "source": [
    "Теперь дополнительно обрабатываем каждую, чтобы привести их к нужному формату и типу данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f6995bb-0ab7-436b-976c-3f00794f2475",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df['hashtags'] = res_df.text.str.contains('#[A-Za-z]+', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3760731c-c892-4947-8f66-3be84b54b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df['retweets'] = res_df['retweets'].str.findall('\\d+ reposts').explode().str.rstrip(' reposts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "23e3103f-f0d9-47de-9d1e-c28b9f5040b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df['favorites'] = res_df['favorites'].str.findall('\\d+ bookmarks').explode().str.rstrip(' bookmarks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae478f0-1d9c-4d31-862c-47ebb7e6b941",
   "metadata": {},
   "source": [
    "Теперь еще обработка дат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "29fd1fd4-e569-4959-b2a4-1a11fc8ed82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-01-02 21:00:00')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime('9:00:00') + pd.Timedelta(hours=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8b54e735-2ae8-4c68-b020-79023eef6196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_date(row_list):\n",
    "    if len(row_list) == 2:\n",
    "        time = row_list[0]\n",
    "        date = row_list[1]\n",
    "        \n",
    "    elif len(row_list) == 1:\n",
    "        time = '00:00:00'\n",
    "        date = row_list[0]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "    try:\n",
    "        # Обработаем время\n",
    "        if 'AM' in time:\n",
    "            time = time.strip(' AM') + ':00'\n",
    "        elif 'PM' in time:\n",
    "            time = time.strip(' PM') + ':00'\n",
    "            time = str(pd.to_datetime(time) + pd.Timedelta(hours=12))[11:]\n",
    "    \n",
    "        # Обработаем дату\n",
    "        for mon in replace_dict:\n",
    "            if mon in date:\n",
    "                date = date.replace(mon, replace_dict.get(mon))\n",
    "    \n",
    "        date = date.replace(' ', '-').replace(',', '')\n",
    "    \n",
    "        return pd.to_datetime(date + ' ' + time, format='%m-%d-%Y %H:%M:%S')\n",
    "    except:\n",
    "        return np.nan  # invalid format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b16af33a-e34f-40d8-8241-02c7c045a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df['date'] = res_df['date'].str.split(' · ').apply(lambda x : transform_date(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4911e19c-1656-4a33-935d-97528cb65b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_csv('tweet_dataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter-scraping-env",
   "language": "python",
   "name": "twitter-scraping-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
