{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00e6483b-c1ca-4795-bd7d-bfce6b211b7d",
   "metadata": {},
   "source": [
    "### Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26205f57-a375-4e49-9014-425d39a53cb2",
   "metadata": {},
   "source": [
    "**Импорт зависимостей**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4635209-a277-443d-851e-1a33d4fe9894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/course-work-venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from html import unescape\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccecf3ea-85ad-42e5-80bb-df876f7224b0",
   "metadata": {},
   "source": [
    "**Как мы получаем `elon_musk_tweets_after_eda.csv`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d9937c-13d8-47e4-bd0e-d8a91432d99a",
   "metadata": {},
   "source": [
    "Открываем наш размеченный датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ddd7364-a7b4-46c1-80c6-9dc45a29b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('elon_musk_tweets_labeled.parquet').rename(columns={'feeling_auto': 'feeling'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205172ee-0d10-40a3-ac09-427b22704d67",
   "metadata": {},
   "source": [
    "Все необходимые преобразования:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b24cb64f-796a-4c74-a4e8-c81a7b93e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['id', 'user_created'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2b3a3f9-a403-4cf0-93d2-e411895a7219",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd03ba5e-af13-4103-a2ff-6b538fb7e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "924c19f0-dcb1-4f67-bc9b-db51da1ba1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['hashtags', 'user_name', 'is_retweet'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c71d7629-633d-4bd9-9904-1b79633a9cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dff4e86d-4502-478e-b734-bdef0edfa599",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour'] = df['date'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "933eeabd-c4e6-4200-9069-47471622897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweets_per_hour'] = df['hour'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f3aabc5-668d-4960-82bb-8d2ed7284a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = df['date'].dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4af1980-9146-4cce-ad07-006498c43848",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month_year'] = df['date'].dt.to_period('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b178a85c-3179-48d5-9c70-ea61fcd5408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['hour', 'tweets_per_hour', 'month_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fd46a48-9704-42cf-919e-04a63a2fe446",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['user_verified'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a499d34-251a-42d0-b2b3-b0d46414e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'text': 'tweet'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb6fafcf-880e-4e89-acbe-9eb1a821d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'] = df['tweet'].apply(unescape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1d9f417-a0fc-4f93-8092-ae3e712813c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'] = df['tweet'].str.replace(r'#\\b(1|3)\\b', r'\\1', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4e08a68-cd24-4c8b-9d2a-5949b6fd4be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функция для извлечения смайлов из текста\n",
    "def extract_emojis(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"\n",
    "        \"\\U0001F300-\\U0001F5FF\"\n",
    "        \"\\U0001F680-\\U0001F6FF\"\n",
    "        \"\\U0001F700-\\U0001F77F\"\n",
    "        \"\\U0001F780-\\U0001F7FF\"\n",
    "        \"\\U0001F800-\\U0001F8FF\"\n",
    "        \"\\U0001F900-\\U0001F9FF\"\n",
    "        \"\\U0001FA00-\\U0001FAFF\"\n",
    "        \"\\U00002700-\\U000027BF\"\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\"\n",
    "    )\n",
    "    return emoji_pattern.findall(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45adedb7-a088-4a39-8f60-0c91666faa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emojis'] = df['tweet'].apply(lambda x: ''.join(extract_emojis(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5772d18-eb61-4c64-8c5e-0d9c764dcc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функции для подсчета количества элементов в твите\n",
    "def count_emails(tweet):\n",
    "    return len(re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', tweet))\n",
    "\n",
    "def count_nicknames(tweet):\n",
    "    return len(re.findall(r'@([A-Za-z0-9_]{1,})', tweet))\n",
    "\n",
    "def count_urls(tweet):\n",
    "    return len(re.findall(r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})', tweet))\n",
    "\n",
    "def count_hashtags(tweet):\n",
    "    return len(re.findall(r'#\\w+', tweet))\n",
    "\n",
    "def count_emojis(tweet):\n",
    "    return len(re.findall(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U00002700-\\U000027BF]', tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0b8aaa2-14df-4c71-9fbf-a0e3c72405f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['email_count'] = df['tweet'].apply(count_emails)\n",
    "df['nickname_count'] = df['tweet'].apply(count_nicknames)\n",
    "df['url_count'] = df['tweet'].apply(count_urls)\n",
    "df['hashtag_count'] = df['tweet'].apply(count_hashtags)\n",
    "df['emoji_count'] = df['tweet'].apply(count_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d494ac1-7f1d-4438-bb9e-bcb0c5da84b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'] = df['tweet'].str.replace(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', '', regex=True) #Удаляем emails\n",
    "df['tweet'] = df['tweet'].str.replace(r'@([A-Za-z0-9_]{1,})', '', regex=True) #Удаляем ники\n",
    "df['tweet'] = df['tweet'].str.replace(r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})', '', regex=True) #Удаляем urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "165caec8-a499-4969-8bcc-9edf20a71275",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'] = df['tweet'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6133bde5-8afe-4714-8d5a-23681c8a0a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_tweet'] = df['tweet'].str.replace(r'[^a-zA-Z\\s]', '', regex=True) #Удаляем знаки препинания и цифры и оставляем только буквы и пробелы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "737ed463-8803-435c-972d-a81bf9fd26a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_tweet'] = df['cleaned_tweet'].str.replace(r'\\s+', ' ', regex=True).str.strip()  #Удаляем лишние пробелы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84b3c456-5a09-4a8a-85a8-0ed65618dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f135d9b-164b-4178-9576-e2f5666928f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt_tab')\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a53ebc87-b674-4a29-beb4-e6dd8740a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['cleaned_tweet'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "631318ed-d9f2-4412-a071-bc1d6028ba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_stopwords(tokens):\n",
    "    cleaned = []\n",
    "    for word in tokens:\n",
    "        if word not in stop_words:\n",
    "            cleaned.append(word)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a9cd651-db8f-4bca-a679-911191fc4df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_tokens'] = df['tokens'].apply(delete_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc7e5083-42f8-405f-b1f4-4d38131c76f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0778957-554b-44a2-bb09-d5fac68f53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokens(tokens):\n",
    "    lemmatized = []\n",
    "    for word in tokens:\n",
    "        lemmatized.append(lemmatizer.lemmatize(word))\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76a0ae4d-b771-4805-b9e6-6a047ed95b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatized'] = df['cleaned_tokens'].apply(lemmatize_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eeb2f77f-1b39-46df-ba28-095d34de75c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f56bb7df-68c9-473e-a3f7-4d2f6721afe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.36 s, sys: 84.5 ms, total: 4.44 s\n",
      "Wall time: 4.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nlp = spacy.load(\n",
    "    \"en_core_web_sm\",\n",
    "    disable=['parser', 'senter', 'ner', 'lemmatizer'],\n",
    "    enable=['tok2vec', 'tagger', 'attribute_ruler']\n",
    ")\n",
    "df['spacy_lemmatized'] = df.lemmatized.str.join(' ').apply(lambda x : nlp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1211920d-8433-45ef-97d1-31622cd252d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.04 s, sys: 141 ms, total: 4.18 s\n",
      "Wall time: 4.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def spacy_lemmatize_tokens(text):\n",
    "    c = []\n",
    "    doc = nlp(text)\n",
    "    k = [(token.text, token.pos_) for token in doc]\n",
    "    c.extend(k)\n",
    "    return c\n",
    "\n",
    "df['lemmatized_pronouns'] = df['spacy_lemmatized'].apply(spacy_lemmatize_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a3d9724-68d8-4801-a4bc-2ae9cc36489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pos_tags'] = df['lemmatized_pronouns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b8065391-437b-47eb-9996-32753510c113",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['._count'] = df['tweet'].str.count('\\.')\n",
    "df['!_count'] = df['tweet'].str.count('\\!')\n",
    "df['@_count'] = df['tweet'].str.count(r'\\@')\n",
    "df[\"'_count\"] = df['tweet'].str.count('\\'')\n",
    "df[',_count'] = df['tweet'].str.count('\\,')\n",
    "df['/_count'] = df['tweet'].str.count('\\/')\n",
    "df['?_count'] = df['tweet'].str.count(r'\\?')\n",
    "df[';_count'] = df['tweet'].str.count(r'\\;')\n",
    "df['-_count'] = df['tweet'].str.count(r'\\-')\n",
    "df[')_count'] = df['tweet'].str.count(r'\\)')\n",
    "df['#_count'] = df['tweet'].str.count(r'\\#')\n",
    "df['(_count'] = df['tweet'].str.count(r'\\(')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d4f98987-be00-43f7-a9de-057e6cea1d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['char_count'] = df['tweet'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d47e3e71-7f45-4756-ad97-1ec7e72d9020",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'] = df['tweet'].str.split().str.len() #Количество слов в твитах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b6236f71-8e26-44db-9dff-7ef6b597002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count_cleaned'] = df['cleaned_tweet'].str.split().str.len() #Количество слов в твитах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eca7cc91-947f-483d-a310-b40a33eada89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['char_count_cleaned'] = df['cleaned_tweet'].str.len() #Количество символов в твитах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5e1c317e-3e30-4db1-bb87-dec5eaeedf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(index=df.loc[df.lemmatized.apply(lambda x : len(x)) == 0].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dd1cbc9b-356d-4baf-ae10-58d1656b27de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatized_str'] = df['lemmatized'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "62c0b9a6-91d8-45c5-be7d-0dca262d872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_patterns = {\n",
    "\"gon na\": \"gonna\",\n",
    "\"wan na\": \"wanna\",\n",
    "\"do n't\": \"don't\",\n",
    "\"ai n't\": \"ain't\",\n",
    "\"got ta\": \"gotta\"\n",
    "}\n",
    "\n",
    "# Создаем регулярное выражение на основе списка неформальных фраз\n",
    "pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contraction_patterns.keys()) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "# Функция для замены неформальных фраз\n",
    "def replace_contractions(text):\n",
    "    # Проверяем, содержатся ли целевые фразы в тексте\n",
    "    if any(key in text for key in contraction_patterns.keys()):\n",
    "    # Заменяем каждую найденную фразу на нужное значение\n",
    "        return pattern.sub(lambda x: contraction_patterns[x.group(0).lower()], text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "30b21565-17f4-4409-b0b8-e4c16420d3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatized_str'] = df['lemmatized_str'].apply(replace_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c5aae3c8-e3d4-4898-b72b-2b8f09ad37a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour'] = df['date'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0e88516b-9ae8-465f-aec4-dd2a2c7f4b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'] = df['date'].dt.to_period('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "389b113c-c7de-4c69-bdd2-a20549e1d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'] = df['month'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2ee05b6d-4c42-4f10-982d-1bdaa1dfff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['year', 'month']] = df['month'].str.split('-', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b6a5ecfa-ca0e-45ba-94a1-8ab677bfbdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df['year'].astype(int)\n",
    "df['month'] = df['month'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8f7297b5-945c-4476-9680-3cebbd3510d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['favourites'] = df['favorites']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611d5ac7-48a0-48ee-b3db-a39ec80200ae",
   "metadata": {},
   "source": [
    "Выгрузка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9c40fb9f-b8d2-4048-82dd-e956247f8bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "needed_cols = ['user_location',\n",
    " 'user_description',\n",
    " 'user_friends',\n",
    " 'date',\n",
    " 'tweet',\n",
    " 'favourites',\n",
    " 'feeling',\n",
    " 'month',\n",
    " 'word_count',\n",
    " 'char_count',\n",
    " '._count',\n",
    " '!_count',\n",
    " '@_count',\n",
    " \"'_count\",\n",
    " ',_count',\n",
    " '/_count',\n",
    " '?_count',\n",
    " ';_count',\n",
    " '-_count',\n",
    " ')_count',\n",
    " '#_count',\n",
    " '(_count',\n",
    " 'emojis',\n",
    " 'email_count',\n",
    " 'nickname_count',\n",
    " 'url_count',\n",
    " 'hashtag_count',\n",
    " 'emoji_count',\n",
    " 'cleaned_tweet',\n",
    " 'word_count_cleaned',\n",
    " 'char_count_cleaned',\n",
    " 'tokens',\n",
    " 'cleaned_tokens',\n",
    " 'lemmatized',\n",
    " 'spacy_lemmatized',\n",
    " 'lemmatized_pronouns',\n",
    " 'pos_tags',\n",
    " 'lemmatized_str',\n",
    " 'hour',\n",
    " 'year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "784e5df0-70ec-4858-bd65-7bd2de7b80c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, needed_cols].to_csv('elon_musk_tweets_after_eda.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73200d1f-9a54-41fc-9dfa-f04186d504d4",
   "metadata": {},
   "source": [
    "**Что мы делаем с `elon_musk_tweets_after_eda.csv`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "af984a72-cdb3-478e-b2c0-e7d23256459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_date(df):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    return df.sort_values(by='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d0703969-7708-4ed0-bc23-00e2baea05c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_fillna(df):\n",
    "    cols_to_fill = ['user_location', 'user_description', 'emojis']\n",
    "    df.loc[:, cols_to_fill] = df.loc[:, cols_to_fill].fillna('None')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6856246e-7bc2-42fd-accc-f7c7bbc9d46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_useless_cols(df):\n",
    "    useless_cols = [\n",
    "        'lemmatized_pronouns', \n",
    "        'tweet', \n",
    "        'cleaned_tweet', \n",
    "        'tokens', \n",
    "        'cleaned_tokens',\n",
    "        'word_count', \n",
    "        'char_count', \n",
    "        'emojis',\n",
    "        'word_count_cleaned', \n",
    "        'spacy_lemmatized', \n",
    "        'pos_tags', \n",
    "        'lemmatized', \n",
    "        'email_count', \n",
    "        'hashtag_count',\n",
    "        'month', \n",
    "        'year', \n",
    "        'hour'\n",
    "    ]\n",
    "    return df.drop(columns=useless_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2f6564-07a4-43a3-988b-6af0a866a5c2",
   "metadata": {},
   "source": [
    "Читаем данные и делаем базовые преобразования:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "760c270e-46e2-4d23-a93a-b00544f2e51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('elon_musk_tweets_after_eda.csv', encoding='utf-8').drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e05cf88f-7ddf-457a-8d08-9db36f7268ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = transform_date(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "47d0e28d-000f-493a-9a85-9fd29c48406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_fillna(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5c641d41-5f0e-4c49-853b-2a5672cba693",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_useless_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b7eaf73a-8e91-4cad-aed4-77ab8a4fbf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_encoded = pd.get_dummies(df, columns=['user_location', 'user_description'], drop_first=False)\n",
    "\n",
    "# encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "# encoder.fit(df.loc[:, ['user_location', 'user_description']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a479ae-b5fa-4edf-b78a-61fa16f194cf",
   "metadata": {},
   "source": [
    "Кодируем категориальные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a38ddaef-f518-4adb-b156-a0150422a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categoric(df):\n",
    "    with open('encoder.pkl', 'rb') as f:\n",
    "        encoder = pickle.load(f)\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        data=encoder.transform(df.loc[:, ['user_location', 'user_description']]),  # Обучены именно на этих двух колонках\n",
    "        index=df.index,\n",
    "        columns=encoder.get_feature_names_out()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "50dfee9d-d924-4d20-94ba-aefbc8058f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = pd.concat([df.drop(columns=['user_location', 'user_description']), encode_categoric(df)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c20918-1376-4a23-a600-c51834c4801a",
   "metadata": {},
   "source": [
    "Разбиваем на `train/val/test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "778906f7-02b6-4c17-b086-2f98410d2fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cutoff = df_encoded['date'].quantile(0.6)  # Дата, отсекающая 60% наблюдений\n",
    "val_cutoff = df_encoded['date'].quantile(0.8)\n",
    "\n",
    "# Получаем три датафрейма\n",
    "train_df = df_encoded[df_encoded['date'] <= train_cutoff]\n",
    "val_df = df_encoded[(df_encoded['date'] > train_cutoff) & (df['date'] <= val_cutoff)]\n",
    "test_df = df_encoded[df_encoded['date'] > val_cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c001bec3-4285-461e-94f6-ef763456dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=['feeling', 'date'])\n",
    "y_train = train_df['feeling']\n",
    "\n",
    "X_val = val_df.drop(columns=['feeling', 'date'])\n",
    "y_val = val_df['feeling']\n",
    "\n",
    "X_test = test_df.drop(columns=['feeling', 'date'])\n",
    "y_test = test_df['feeling']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ec2f8b-6cca-470c-bd3c-f17cf98967fe",
   "metadata": {},
   "source": [
    "Векторизуем твиты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6b5b92d9-b6e9-4dbc-b7a8-2bd5078f1e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer()\n",
    "# vectorizer.fit(X_train['lemmatized_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "622c28a4-fc51-4e14-b8bc-05f09945cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vectorizer.pkl', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2bb5a683-c10e-4551-9a98-5f5cda215182",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lemmatized = vectorizer.transform(X_train['lemmatized_str'])\n",
    "X_val_lemmatized = vectorizer.transform(X_val['lemmatized_str'])\n",
    "X_test_lemmatized = vectorizer.transform(X_test['lemmatized_str'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b833b55-d85b-4331-bc70-9f5533ee0051",
   "metadata": {},
   "source": [
    "Приводим все числовые фичи к единой шкале:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "73b36a0e-a1fb-45e4-9b67-cce024211f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "925ab28b-154f-4f59-83c9-6c02ce4e7d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b8211931-3a32-4e25-a5e8-5cb7b45fe262",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "372d8616-a816-4c40-bacc-7d4a80a1422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Масштабируем числовые признаки\n",
    "X_train_numerical = scaler.transform(X_train[numerical_features])\n",
    "X_val_numerical = scaler.transform(X_val[numerical_features])\n",
    "X_test_numerical = scaler.transform(X_test[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d2e9df82-7c21-476c-a09a-278d7060c195",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined = hstack([X_train_lemmatized, X_train_numerical])\n",
    "X_val_combined = hstack([X_val_lemmatized, X_val_numerical])\n",
    "X_test_combined = hstack([X_test_lemmatized, X_test_numerical])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python, course work",
   "language": "python",
   "name": "course-work-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
